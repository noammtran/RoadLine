{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96891df8",
   "metadata": {},
   "source": [
    "# Road Line Segmentation (Simplified)\n",
    "\n",
    "This notebook trains a small U-Net model for road line segmentation with a concise training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f0edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision.transforms import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"dataset\")\n",
    "MODEL_DIR = Path(\"Model\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_PATH = MODEL_DIR / \"roadline_unet_best.pth\"\n",
    "\n",
    "IMAGE_SIZE: Tuple[int, int] = (256, 256)\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def list_pairs(split: str) -> List[Tuple[Path, Path]]:\n",
    "    image_dir = DATA_ROOT / split / \"images\"\n",
    "    mask_dir = DATA_ROOT / split / \"masks\"\n",
    "    image_files = sorted(image_dir.glob(\"*.*\"))\n",
    "    mask_files = sorted(mask_dir.glob(\"*.*\"))\n",
    "    if len(image_files) != len(mask_files):\n",
    "        raise ValueError(\n",
    "            f\"Found {len(image_files)} images but {len(mask_files)} masks for split '{split}'.\"\n",
    "        )\n",
    "    return list(zip(image_files, mask_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ee47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadLineDataset(Dataset):\n",
    "    def __init__(self, pairs: List[Tuple[Path, Path]], image_size: Tuple[int, int], augment: bool = False) -> None:\n",
    "        self.pairs = pairs\n",
    "        self.image_size = image_size\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_path, mask_path = self.pairs[index]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.augment and random.random() < 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "\n",
    "        image = TF.resize(image, self.image_size, interpolation=InterpolationMode.BILINEAR)\n",
    "        mask = TF.resize(mask, self.image_size, interpolation=InterpolationMode.NEAREST)\n",
    "\n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "        mask_array = (np.array(mask) > 0).astype(\"int64\")\n",
    "        mask_tensor = torch.from_numpy(mask_array)\n",
    "        return image, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082f16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels: int, skip_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(out_channels + skip_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = self.up(x1)\n",
    "        diff_y = x2.size(2) - x1.size(2)\n",
    "        diff_x = x2.size(3) - x1.size(3)\n",
    "        if diff_y != 0 or diff_x != 0:\n",
    "            x1 = F.pad(\n",
    "                x1,\n",
    "                [diff_x // 2, diff_x - diff_x // 2, diff_y // 2, diff_y - diff_y // 2],\n",
    "            )\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, num_classes: int = 2, base_channels: int = 32) -> None:\n",
    "        super().__init__()\n",
    "        self.inc = DoubleConv(in_channels, base_channels)\n",
    "        self.down1 = Down(base_channels, base_channels * 2)\n",
    "        self.down2 = Down(base_channels * 2, base_channels * 4)\n",
    "        self.down3 = Down(base_channels * 4, base_channels * 8)\n",
    "        self.down4 = Down(base_channels * 8, base_channels * 16)\n",
    "        self.up1 = Up(base_channels * 16, base_channels * 8, base_channels * 8)\n",
    "        self.up2 = Up(base_channels * 8, base_channels * 4, base_channels * 4)\n",
    "        self.up3 = Up(base_channels * 4, base_channels * 2, base_channels * 2)\n",
    "        self.up4 = Up(base_channels * 2, base_channels, base_channels)\n",
    "        self.outc = OutConv(base_channels, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        return self.outc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(logits: torch.Tensor, masks: torch.Tensor) -> float:\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    intersection = torch.logical_and(preds == 1, masks == 1).float().sum()\n",
    "    union = torch.logical_or(preds == 1, masks == 1).float().sum()\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return float((intersection / union).item())\n",
    "\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, optimizer: Adam, criterion: nn.Module) -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    total_samples = 0\n",
    "    for images, masks in loader:\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_iou += compute_iou(logits.detach(), masks) * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "    denom = max(total_samples, 1)\n",
    "    return total_loss / denom, total_iou / denom\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    total_samples = 0\n",
    "    for images, masks in loader:\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, masks)\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_iou += compute_iou(logits, masks) * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "    denom = max(total_samples, 1)\n",
    "    return total_loss / denom, total_iou / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ce595",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "train_pairs = list_pairs(\"train\")\n",
    "val_pairs = list_pairs(\"valid\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    RoadLineDataset(train_pairs, IMAGE_SIZE, augment=True),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    RoadLineDataset(val_pairs, IMAGE_SIZE, augment=False),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "model = UNet().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6088ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_iou = 0.0\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss, train_iou = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_iou = evaluate(model, val_loader, criterion)\n",
    "    if val_iou > best_val_iou:\n",
    "        best_val_iou = val_iou\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"Saved checkpoint to {MODEL_PATH}\")\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} iou={train_iou:.3f} \"\n",
    "        f\"| val_loss={val_loss:.4f} iou={val_iou:.3f}\"\n",
    "    )\n",
    "\n",
    "print(f\"Best validation IoU: {best_val_iou:.3f}\")\n",
    "print(f\"Best model checkpoint: {MODEL_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}