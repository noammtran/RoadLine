{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "97524dc0",
      "metadata": {},
      "source": [
        "# Road Line Segmentation Training\n",
        "\n",
        "This notebook mirrors `simple_unet_training.py` and splits the code into logical sections for interactive execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "3c368799",
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse\n",
        "import random\n",
        "import math\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Sequence, Tuple\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torchvision.transforms import functional as TF\n",
        "\n",
        "IMAGE_EXTENSIONS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4288da74",
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def resolve_device(device_str: str) -> torch.device:\n",
        "    device_str = device_str.strip()\n",
        "    if device_str.lower() == \"auto\":\n",
        "        device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        return torch.device(device_type)\n",
        "    try:\n",
        "        device = torch.device(device_str)\n",
        "    except (RuntimeError, ValueError) as exc:\n",
        "        raise ValueError(f\"Could not interpret device '{device_str}': {exc}\") from exc\n",
        "    if device.type == \"cuda\" and not torch.cuda.is_available():\n",
        "        raise ValueError(\"CUDA requested via --device but no GPU is available.\")\n",
        "    return device\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "05c16977",
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_image_mask_pairs(images_dir: Path, masks_dir: Path) -> List[Tuple[Path, Path]]:\n",
        "    if not images_dir.exists() or not masks_dir.exists():\n",
        "        raise FileNotFoundError(f\"Missing directory: {images_dir} or {masks_dir}\")\n",
        "    image_paths = [\n",
        "        p for p in images_dir.iterdir() if p.suffix.lower() in IMAGE_EXTENSIONS\n",
        "    ]\n",
        "    mask_paths = [\n",
        "        p for p in masks_dir.iterdir() if p.suffix.lower() in IMAGE_EXTENSIONS\n",
        "    ]\n",
        "    mask_lookup: Dict[str, Path] = {}\n",
        "    for mask_path in mask_paths:\n",
        "        stem = mask_path.stem\n",
        "        candidate_keys = {stem}\n",
        "        for suffix in (\"_mask\", \"-mask\", \"_label\", \"-label\"):\n",
        "            if stem.endswith(suffix):\n",
        "                candidate_keys.add(stem[: -len(suffix)])\n",
        "        for key in candidate_keys:\n",
        "            mask_lookup.setdefault(key, mask_path)\n",
        "    pairs: List[Tuple[Path, Path]] = []\n",
        "    missing_images: List[str] = []\n",
        "    for image_path in sorted(image_paths):\n",
        "        base = image_path.stem\n",
        "        candidates = [\n",
        "            base,\n",
        "            f\"{base}_mask\",\n",
        "            f\"{base}-mask\",\n",
        "            f\"{base}_label\",\n",
        "            f\"{base}-label\",\n",
        "        ]\n",
        "        mask_path = None\n",
        "        for candidate in candidates:\n",
        "            mask_path = mask_lookup.get(candidate)\n",
        "            if mask_path is not None:\n",
        "                break\n",
        "        if mask_path is None:\n",
        "            missing_images.append(image_path.name)\n",
        "            continue\n",
        "        pairs.append((image_path, mask_path))\n",
        "    if not pairs:\n",
        "        raise RuntimeError(\n",
        "            f\"No image/mask pairs were found in {images_dir} and {masks_dir}\"\n",
        "        )\n",
        "    if missing_images:\n",
        "        preview = \", \".join(missing_images[:5])\n",
        "        print(\n",
        "            f\"Warning: {len(missing_images)} image files did not have matching masks in {masks_dir}. \"\n",
        "            f\"Examples: {preview}\"\n",
        "        )\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def compute_dataset_statistics(\n",
        "    pairs: Sequence[Tuple[Path, Path]],\n",
        "    image_size: Tuple[int, int],\n",
        ") -> Dict[str, object]:\n",
        "    class_counts = np.zeros(2, dtype=np.int64)\n",
        "    channel_sum = np.zeros(3, dtype=np.float64)\n",
        "    channel_sq_sum = np.zeros(3, dtype=np.float64)\n",
        "    total_pixels = 0\n",
        "    positive_pixel_ratios: List[float] = []\n",
        "    for image_path, mask_path in pairs:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "        image = TF.resize(image, image_size, interpolation=InterpolationMode.BILINEAR)\n",
        "        mask = TF.resize(mask, image_size, interpolation=InterpolationMode.NEAREST)\n",
        "        image_tensor = TF.to_tensor(image)\n",
        "        flat = image_tensor.view(3, -1)\n",
        "        channel_sum += flat.sum(dim=1).double().numpy()\n",
        "        channel_sq_sum += (flat**2).sum(dim=1).double().numpy()\n",
        "        total_pixels += flat.shape[1]\n",
        "        mask_array = np.array(mask, dtype=np.uint8)\n",
        "        binary_mask = (mask_array > 0).astype(np.int64)\n",
        "        foreground = int(binary_mask.sum())\n",
        "        background = int(binary_mask.size - foreground)\n",
        "        class_counts[0] += background\n",
        "        class_counts[1] += foreground\n",
        "        positive_pixel_ratios.append(float(foreground / max(binary_mask.size, 1)))\n",
        "    image_mean = (channel_sum / max(total_pixels, 1)).tolist()\n",
        "    variance = channel_sq_sum / max(total_pixels, 1) - np.square(image_mean)\n",
        "    image_std = np.sqrt(np.clip(variance, a_min=1e-6, a_max=None)).tolist()\n",
        "    total_pixels_seen = int(class_counts.sum())\n",
        "    if total_pixels_seen == 0:\n",
        "        class_frequencies = [1.0, 0.0]\n",
        "    else:\n",
        "        class_frequencies = (class_counts / total_pixels_seen).tolist()\n",
        "    positive_ratios_np = (\n",
        "        np.array(positive_pixel_ratios, dtype=np.float64)\n",
        "        if positive_pixel_ratios\n",
        "        else np.array([0.0])\n",
        "    )\n",
        "    ratio_summary = {\n",
        "        \"mean\": float(positive_ratios_np.mean()),\n",
        "        \"std\": float(positive_ratios_np.std()),\n",
        "        \"min\": float(positive_ratios_np.min()),\n",
        "        \"max\": float(positive_ratios_np.max()),\n",
        "        \"p10\": float(np.percentile(positive_ratios_np, 10)),\n",
        "        \"p50\": float(np.percentile(positive_ratios_np, 50)),\n",
        "        \"p90\": float(np.percentile(positive_ratios_np, 90)),\n",
        "    }\n",
        "    hist_counts, hist_edges = np.histogram(\n",
        "        positive_ratios_np, bins=np.linspace(0.0, 1.0, 11)\n",
        "    )\n",
        "    hist_bins_centers = ((hist_edges[:-1] + hist_edges[1:]) / 2).tolist()\n",
        "    return {\n",
        "        \"num_samples\": len(pairs),\n",
        "        \"image_mean\": image_mean,\n",
        "        \"image_std\": image_std,\n",
        "        \"class_counts\": class_counts.tolist(),\n",
        "        \"class_frequencies\": class_frequencies,\n",
        "        \"image_size\": list(image_size),\n",
        "        \"positive_pixel_ratios\": positive_pixel_ratios,\n",
        "        \"positive_pixel_ratio_summary\": ratio_summary,\n",
        "        \"positive_pixel_ratio_histogram\": {\n",
        "            \"bins\": hist_bins_centers,\n",
        "            \"counts\": hist_counts.tolist(),\n",
        "        },\n",
        "    }\n",
        "\n",
        "\n",
        "def log_dataset_statistics(dataset_name: str, stats: Dict[str, object]) -> None:\n",
        "    print(f\"\\n[{dataset_name}] samples: {stats['num_samples']}\")\n",
        "    print(f\"[{dataset_name}] image mean: {stats['image_mean']}\")\n",
        "    print(f\"[{dataset_name}] image std:  {stats['image_std']}\")\n",
        "    print(\n",
        "        f\"[{dataset_name}] class counts (background, foreground): {stats['class_counts']}\"\n",
        "    )\n",
        "    print(f\"[{dataset_name}] class frequencies: {stats['class_frequencies']}\")\n",
        "    ratio_summary = stats.get(\"positive_pixel_ratio_summary\")\n",
        "    if ratio_summary:\n",
        "        print(\n",
        "            f\"[{dataset_name}] positive pixel ratio mean={ratio_summary['mean']:.4f} \"\n",
        "            f\"std={ratio_summary['std']:.4f} \"\n",
        "            f\"min={ratio_summary['min']:.4f} \"\n",
        "            f\"p50={ratio_summary['p50']:.4f} \"\n",
        "            f\"max={ratio_summary['max']:.4f}\"\n",
        "        )\n",
        "    histogram = stats.get(\"positive_pixel_ratio_histogram\")\n",
        "    if histogram:\n",
        "        bins = histogram[\"bins\"]\n",
        "        counts = histogram[\"counts\"]\n",
        "        formatted = \", \".join(\n",
        "            f\"{bin_center:.2f}:{count}\" for bin_center, count in zip(bins, counts)\n",
        "        )\n",
        "        print(\n",
        "            f\"[{dataset_name}] positive pixel ratio histogram (bin:count) -> {formatted}\"\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "cf2d2118",
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_random_augmentations(\n",
        "    image: Image.Image, mask: Image.Image\n",
        ") -> Tuple[Image.Image, Image.Image]:\n",
        "    if random.random() < 0.5:\n",
        "        image = TF.hflip(image)\n",
        "        mask = TF.hflip(mask)\n",
        "    if random.random() < 0.1:\n",
        "        image = TF.vflip(image)\n",
        "        mask = TF.vflip(mask)\n",
        "    if random.random() < 0.3:\n",
        "        angle = random.uniform(-10.0, 10.0)\n",
        "        image = TF.rotate(\n",
        "            image,\n",
        "            angle,\n",
        "            interpolation=InterpolationMode.BILINEAR,\n",
        "            fill=(0, 0, 0),\n",
        "        )\n",
        "        mask = TF.rotate(\n",
        "            mask,\n",
        "            angle,\n",
        "            interpolation=InterpolationMode.NEAREST,\n",
        "            fill=0,\n",
        "        )\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "class RoadLineDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        pairs: Sequence[Tuple[Path, Path]],\n",
        "        image_size: Tuple[int, int],\n",
        "        augment: bool,\n",
        "        mean: Sequence[float],\n",
        "        std: Sequence[float],\n",
        "    ) -> None:\n",
        "        self.pairs = list(pairs)\n",
        "        self.image_size = image_size\n",
        "        self.augment = augment\n",
        "        self.mean = list(mean)\n",
        "        self.std = list(std)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        image_path, mask_path = self.pairs[index]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "        if self.augment:\n",
        "            image, mask = apply_random_augmentations(image, mask)\n",
        "        image = TF.resize(\n",
        "            image, self.image_size, interpolation=InterpolationMode.BILINEAR\n",
        "        )\n",
        "        mask = TF.resize(mask, self.image_size, interpolation=InterpolationMode.NEAREST)\n",
        "        image_tensor = TF.to_tensor(image)\n",
        "        image_tensor = TF.normalize(image_tensor, mean=self.mean, std=self.std)\n",
        "        mask_array = np.array(mask, dtype=np.uint8)\n",
        "        binary_mask = (mask_array > 0).astype(np.int64)\n",
        "        mask_tensor = torch.from_numpy(binary_mask).long()\n",
        "        return image_tensor, mask_tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "06d85346",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels: int, skip_channels: int, out_channels: int) -> None:\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(out_channels + skip_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
        "        x1 = self.up(x1)\n",
        "        diff_y = x2.size(2) - x1.size(2)\n",
        "        diff_x = x2.size(3) - x1.size(3)\n",
        "        if diff_y != 0 or diff_x != 0:\n",
        "            x1 = F.pad(\n",
        "                x1,\n",
        "                [diff_x // 2, diff_x - diff_x // 2, diff_y // 2, diff_y - diff_y // 2],\n",
        "            )\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels: int = 3, num_classes: int = 2, base_channels: int = 32\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.inc = DoubleConv(in_channels, base_channels)\n",
        "        self.down1 = Down(base_channels, base_channels * 2)\n",
        "        self.down2 = Down(base_channels * 2, base_channels * 4)\n",
        "        self.down3 = Down(base_channels * 4, base_channels * 8)\n",
        "        self.down4 = Down(base_channels * 8, base_channels * 16)\n",
        "        self.up1 = Up(base_channels * 16, base_channels * 8, base_channels * 8)\n",
        "        self.up2 = Up(base_channels * 8, base_channels * 4, base_channels * 4)\n",
        "        self.up3 = Up(base_channels * 4, base_channels * 2, base_channels * 2)\n",
        "        self.up4 = Up(base_channels * 2, base_channels, base_channels)\n",
        "        self.outc = OutConv(base_channels, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        return self.outc(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "74f985db",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TverskyLoss(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        alpha: float = 0.7,\n",
        "        beta: float = 0.3,\n",
        "        smooth: float = 1.0,\n",
        "        from_logits: bool = True,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.smooth = smooth\n",
        "        self.from_logits = from_logits\n",
        "\n",
        "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        if logits.dim() == 4 and logits.size(1) > 1:\n",
        "            probs = torch.softmax(logits, dim=1)[:, 1, ...]\n",
        "        else:\n",
        "            probs = (\n",
        "                torch.sigmoid(logits.squeeze(1))\n",
        "                if self.from_logits\n",
        "                else logits.squeeze(1)\n",
        "            )\n",
        "        probs = probs.clamp(min=1e-7, max=1 - 1e-7)\n",
        "        targets = targets.float()\n",
        "        tp = (probs * targets).sum(dim=(1, 2))\n",
        "        fp = (probs * (1.0 - targets)).sum(dim=(1, 2))\n",
        "        fn = ((1.0 - probs) * targets).sum(dim=(1, 2))\n",
        "        score = (tp + self.smooth) / (\n",
        "            tp + self.alpha * fp + self.beta * fn + self.smooth\n",
        "        )\n",
        "        return 1.0 - score.mean()\n",
        "\n",
        "class SegmentationCriterion(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        class_counts: Sequence[float],\n",
        "        smooth: float = 1.0,\n",
        "        w_bg: float = 1.5,\n",
        "        w_fg: float = 1.0,\n",
        "        alpha: float = 0.7,\n",
        "        beta: float = 0.3,\n",
        "        lambda_ce: float = 1.0,\n",
        "        lambda_tv: float = 1.0,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        counts = torch.tensor(class_counts, dtype=torch.float32).clamp_min(1.0)\n",
        "        total = counts.sum()\n",
        "        base_weights = total / (len(counts) * counts)\n",
        "        custom_weights = torch.tensor([w_bg, w_fg], dtype=base_weights.dtype)\n",
        "        if custom_weights.numel() != base_weights.numel():\n",
        "            custom_weights = torch.ones_like(base_weights)\n",
        "        combined_weights = base_weights * custom_weights\n",
        "        combined_weights = combined_weights / combined_weights.mean()\n",
        "        self.register_buffer(\"class_weights\", combined_weights)\n",
        "        self.smooth = smooth\n",
        "        self.lambda_ce = lambda_ce\n",
        "        self.lambda_tv = lambda_tv\n",
        "        self.secondary_loss = TverskyLoss(\n",
        "            alpha=alpha, beta=beta, smooth=smooth, from_logits=True\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        logits: torch.Tensor,\n",
        "        targets: torch.Tensor,\n",
        "        return_components: bool = False,\n",
        "    ) -> torch.Tensor | Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
        "        ce_loss = (\n",
        "            F.cross_entropy(logits, targets, weight=self.class_weights)\n",
        "            if self.lambda_ce > 0\n",
        "            else logits.new_tensor(0.0)\n",
        "        )\n",
        "        secondary = self.secondary_loss(logits, targets)\n",
        "        total_loss = self.lambda_ce * ce_loss + self.lambda_tv * secondary\n",
        "        if return_components:\n",
        "            return total_loss, {\n",
        "                \"ce\": ce_loss.detach(),\n",
        "                \"tversky\": secondary.detach(),\n",
        "            }\n",
        "        return total_loss\n",
        "\n",
        "# === Phu luc / Optional: cac loss khac (tham khao, khong thuc thi) ===\n",
        "_OPTIONAL_LOSSES_DOC = '''\n",
        "class CrossEntropyDiceLoss(nn.Module):\n",
        "    def __init__(self, class_counts: Sequence[float], smooth: float = 1.0) -> None:\n",
        "        super().__init__()\n",
        "        counts = torch.tensor(class_counts, dtype=torch.float32).clamp_min(1.0)\n",
        "        total = counts.sum()\n",
        "        class_weights = total / (len(counts) * counts)\n",
        "        self.register_buffer(\"class_weights\", class_weights)\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        logits: torch.Tensor,\n",
        "        targets: torch.Tensor,\n",
        "        return_components: bool = False,\n",
        "    ) -> torch.Tensor | Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
        "        ce = F.cross_entropy(logits, targets, weight=self.class_weights)\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        one_hot = (\n",
        "            F.one_hot(targets, num_classes=logits.shape[1]).permute(0, 3, 1, 2).float()\n",
        "        )\n",
        "        intersection = (probabilities * one_hot).sum(dim=(0, 2, 3))\n",
        "        total = probabilities.sum(dim=(0, 2, 3)) + one_hot.sum(dim=(0, 2, 3))\n",
        "        dice_loss_per_class = 1.0 - (\n",
        "            (2.0 * intersection + self.smooth) / (total + self.smooth)\n",
        "        )\n",
        "        dice_loss = dice_loss_per_class.mean()\n",
        "        total_loss = ce + dice_loss\n",
        "        if return_components:\n",
        "            return total_loss, {\"ce\": ce.detach(), \"dice\": dice_loss.detach()}\n",
        "        return total_loss\n",
        "\n",
        "class FocalTverskyLoss(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        alpha: float = 0.7,\n",
        "        beta: float = 0.3,\n",
        "        gamma: float = 1.33,\n",
        "        smooth: float = 1.0,\n",
        "        from_logits: bool = True,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        self.smooth = smooth\n",
        "        self.from_logits = from_logits\n",
        "        self.base_loss = TverskyLoss(\n",
        "            alpha=alpha, beta=beta, smooth=smooth, from_logits=from_logits\n",
        "        )\n",
        "\n",
        "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        base_value = self.base_loss(logits, targets)\n",
        "        return torch.pow(base_value.clamp(min=1e-8), self.gamma)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "43d9c61e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def calculate_iou(\n",
        "#     logits: torch.Tensor, targets: torch.Tensor, num_classes: int = 2\n",
        "# ) -> torch.Tensor:\n",
        "#     predictions = torch.argmax(logits, dim=1)\n",
        "#     if targets.ndim == 4:\n",
        "#         ground_truth = torch.argmax(targets, dim=1)\n",
        "#     else:\n",
        "#         ground_truth = targets\n",
        "#     ious: List[torch.Tensor] = []\n",
        "#     for cls in range(num_classes):\n",
        "#         pred_mask = predictions == cls\n",
        "#         gt_mask = ground_truth == cls\n",
        "#         intersection = (pred_mask & gt_mask).sum().float()\n",
        "#         union = pred_mask.sum() + gt_mask.sum() - intersection\n",
        "#         if union == 0:\n",
        "#             continue\n",
        "#         ious.append((intersection + 1e-6) / (union + 1e-6))\n",
        "#     if not ious:\n",
        "#         return torch.tensor(0.0, device=logits.device)\n",
        "#     return torch.stack(ious).mean()\n",
        "#\n",
        "#\n",
        "# def calculate_pixel_accuracy(\n",
        "#     logits: torch.Tensor, targets: torch.Tensor\n",
        "# ) -> torch.Tensor:\n",
        "#     predictions = torch.argmax(logits, dim=1)\n",
        "#     if targets.ndim == 4:\n",
        "#         ground_truth = torch.argmax(targets, dim=1)\n",
        "#     else:\n",
        "#         ground_truth = targets\n",
        "#     correct = (predictions == ground_truth).sum().float()\n",
        "#     total = ground_truth.numel()\n",
        "#     return correct / max(total, 1)\n",
        "#\n",
        "#\n",
        "def _extract_boundary(mask: torch.Tensor) -> torch.Tensor:\n",
        "    mask = mask.float().unsqueeze(1)\n",
        "    inv_mask = 1.0 - mask\n",
        "    dilated_inv = F.max_pool2d(inv_mask, kernel_size=3, stride=1, padding=1)\n",
        "    eroded = 1.0 - dilated_inv\n",
        "    boundary = (mask - eroded).clamp(min=0.0)\n",
        "    return boundary.squeeze(1)\n",
        "\n",
        "\n",
        "def _dilate_mask(mask: torch.Tensor, radius: int) -> torch.Tensor:\n",
        "    if radius <= 0:\n",
        "        return mask\n",
        "    kernel_size = radius * 2 + 1\n",
        "    return F.max_pool2d(\n",
        "        mask.unsqueeze(1), kernel_size=kernel_size, stride=1, padding=radius\n",
        "    ).squeeze(1)\n",
        "\n",
        "\n",
        "def compute_boundary_counts(\n",
        "    predictions: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    tolerance_ratio: float = 0.02,\n",
        ") -> Tuple[float, float, float, float]:\n",
        "    if predictions.numel() == 0:\n",
        "        return 0.0, 0.0, 0.0, 0.0\n",
        "    preds = predictions.detach().to(dtype=torch.float32)\n",
        "    gts = targets.detach().to(dtype=torch.float32)\n",
        "    pred_boundary = _extract_boundary(preds)\n",
        "    gt_boundary = _extract_boundary(gts)\n",
        "    height, width = preds.shape[-2:]\n",
        "    diag = math.sqrt(height**2 + width**2)\n",
        "    radius = max(1, int(round(tolerance_ratio * diag)))\n",
        "    pred_dilated = _dilate_mask(pred_boundary, radius)\n",
        "    gt_dilated = _dilate_mask(gt_boundary, radius)\n",
        "    matches_pred = (pred_boundary * gt_dilated).sum().item()\n",
        "    matches_gt = (gt_boundary * pred_dilated).sum().item()\n",
        "    pred_boundary_pixels = pred_boundary.sum().item()\n",
        "    gt_boundary_pixels = gt_boundary.sum().item()\n",
        "    return matches_pred, matches_gt, pred_boundary_pixels, gt_boundary_pixels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "da4e7667",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MetricsAccumulator:\n",
        "    def __init__(\n",
        "        self, collect_probabilities: bool = False, boundary_tolerance: float = 0.02\n",
        "    ) -> None:\n",
        "        self.collect_probabilities = collect_probabilities\n",
        "        self.boundary_tolerance = boundary_tolerance\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self.tp = 0.0\n",
        "        self.fp = 0.0\n",
        "        self.fn = 0.0\n",
        "        self.tn = 0.0\n",
        "        self.matches_pred = 0.0\n",
        "        self.matches_gt = 0.0\n",
        "        self.pred_boundary_pixels = 0.0\n",
        "        self.gt_boundary_pixels = 0.0\n",
        "        self.pixel_count = 0.0\n",
        "        if self.collect_probabilities:\n",
        "            self.probabilities: List[torch.Tensor] = []\n",
        "            self.labels: List[torch.Tensor] = []\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        logits: torch.Tensor,\n",
        "        targets: torch.Tensor,\n",
        "        threshold: float | None = None,\n",
        "    ) -> None:\n",
        "        with torch.no_grad():\n",
        "            probability_fg: torch.Tensor | None = None\n",
        "            if logits.dim() == 4 and logits.size(1) > 1:\n",
        "                probs = torch.softmax(logits, dim=1)\n",
        "                if logits.size(1) == 2 and threshold is not None:\n",
        "                    probability_fg = probs[:, 1, ...]\n",
        "                    preds = (probability_fg >= threshold).long()\n",
        "                else:\n",
        "                    preds = torch.argmax(probs, dim=1)\n",
        "                    if probs.size(1) > 1:\n",
        "                        probability_fg = probs[:, 1, ...]\n",
        "            else:\n",
        "                logits_flat = logits if logits.dim() == 3 else logits.squeeze(1)\n",
        "                probability_fg = torch.sigmoid(logits_flat)\n",
        "                if threshold is None:\n",
        "                    preds = (probability_fg >= 0.5).long()\n",
        "                else:\n",
        "                    preds = (probability_fg >= threshold).long()\n",
        "            fg_pred = preds == 1\n",
        "            fg_true = targets == 1\n",
        "            bg_true = targets == 0\n",
        "            tp = (fg_pred & fg_true).sum().item()\n",
        "            fp = (fg_pred & ~fg_true).sum().item()\n",
        "            fn = (~fg_pred & fg_true).sum().item()\n",
        "            tn = ((preds == 0) & bg_true).sum().item()\n",
        "            self.tp += tp\n",
        "            self.fp += fp\n",
        "            self.fn += fn\n",
        "            self.tn += tn\n",
        "            self.pixel_count += preds.numel()\n",
        "            matches_pred, matches_gt, pred_boundary_pixels, gt_boundary_pixels = (\n",
        "                compute_boundary_counts(\n",
        "                    fg_pred.float(),\n",
        "                    fg_true.float(),\n",
        "                    tolerance_ratio=self.boundary_tolerance,\n",
        "                )\n",
        "            )\n",
        "            self.matches_pred += matches_pred\n",
        "            self.matches_gt += matches_gt\n",
        "            self.pred_boundary_pixels += pred_boundary_pixels\n",
        "            self.gt_boundary_pixels += gt_boundary_pixels\n",
        "            if self.collect_probabilities:\n",
        "                if probability_fg is not None:\n",
        "                    self.probabilities.append(probability_fg.detach().cpu().reshape(-1))\n",
        "                    self.labels.append(fg_true.detach().cpu().reshape(-1))\n",
        "\n",
        "    def _safe_div(self, numerator: float, denominator: float) -> float:\n",
        "        if denominator <= 0.0:\n",
        "            return 0.0\n",
        "        return numerator / denominator\n",
        "\n",
        "    def compute(self) -> Dict[str, float | Dict[str, float]]:\n",
        "        precision = self._safe_div(self.tp, self.tp + self.fp)\n",
        "        recall = self._safe_div(self.tp, self.tp + self.fn)\n",
        "        f1 = self._safe_div(2 * precision * recall, precision + recall)\n",
        "        iou = self._safe_div(self.tp, self.tp + self.fp + self.fn)\n",
        "        dice = self._safe_div(2 * self.tp, 2 * self.tp + self.fp + self.fn)\n",
        "        pixel_accuracy = self._safe_div(self.tp + self.tn, self.pixel_count)\n",
        "        boundary_precision = self._safe_div(\n",
        "            self.matches_pred, self.pred_boundary_pixels\n",
        "        )\n",
        "        boundary_recall = self._safe_div(self.matches_gt, self.gt_boundary_pixels)\n",
        "        boundary_f1 = self._safe_div(\n",
        "            2 * boundary_precision * boundary_recall,\n",
        "            boundary_precision + boundary_recall,\n",
        "        )\n",
        "        confusion_matrix = {\n",
        "            \"tp\": int(self.tp),\n",
        "            \"fp\": int(self.fp),\n",
        "            \"fn\": int(self.fn),\n",
        "            \"tn\": int(self.tn),\n",
        "        }\n",
        "        return {\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"foreground_iou\": iou,\n",
        "            \"foreground_dice\": dice,\n",
        "            \"pixel_accuracy\": pixel_accuracy,\n",
        "            \"boundary_precision\": boundary_precision,\n",
        "            \"boundary_recall\": boundary_recall,\n",
        "            \"boundary_f1\": boundary_f1,\n",
        "            \"confusion_matrix\": confusion_matrix,\n",
        "        }\n",
        "\n",
        "    def pr_curve(\n",
        "        self, num_thresholds: int = 11\n",
        "    ) -> List[Tuple[float, float, float, float]]:\n",
        "        if not self.collect_probabilities or not self.probabilities:\n",
        "            return []\n",
        "        scores = torch.cat(self.probabilities)\n",
        "        labels = torch.cat(self.labels).float()\n",
        "        thresholds = torch.linspace(0.0, 1.0, steps=num_thresholds)\n",
        "        curve: List[Tuple[float, float, float, float]] = []\n",
        "        for threshold in thresholds:\n",
        "            preds = (scores >= threshold).to(labels.dtype)\n",
        "            tp = (preds * labels).sum().item()\n",
        "            fp = (preds * (1 - labels)).sum().item()\n",
        "            fn = ((1 - preds) * labels).sum().item()\n",
        "            precision = 0.0 if (tp + fp) == 0 else tp / (tp + fp)\n",
        "            recall = 0.0 if (tp + fn) == 0 else tp / (tp + fn)\n",
        "            f1 = (\n",
        "                0.0\n",
        "                if (precision + recall) == 0\n",
        "                else (2 * precision * recall) / (precision + recall)\n",
        "            )\n",
        "            curve.append((float(threshold), precision, recall, f1))\n",
        "        return curve\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience: int, min_delta: float = 0.0) -> None:\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_score: float | None = None\n",
        "        self.counter = 0\n",
        "\n",
        "    def step(self, value: float) -> bool:\n",
        "        if self.best_score is None or value > self.best_score + self.min_delta:\n",
        "            self.best_score = value\n",
        "            self.counter = 0\n",
        "            return False\n",
        "        self.counter += 1\n",
        "        return self.counter >= self.patience\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "f0a0d31e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_validate(args: argparse.Namespace) -> None:\n",
        "    device = resolve_device(args.device)\n",
        "    image_size = tuple(args.image_size)\n",
        "    train_pairs = find_image_mask_pairs(\n",
        "        Path(args.data_root) / \"train\" / \"images\",\n",
        "        Path(args.data_root) / \"train\" / \"masks\",\n",
        "    )\n",
        "    val_pairs = find_image_mask_pairs(\n",
        "        Path(args.data_root) / \"valid\" / \"images\",\n",
        "        Path(args.data_root) / \"valid\" / \"masks\",\n",
        "    )\n",
        "    train_stats = compute_dataset_statistics(train_pairs, image_size)\n",
        "    val_stats = compute_dataset_statistics(val_pairs, image_size)\n",
        "    log_dataset_statistics(\"train\", train_stats)\n",
        "    log_dataset_statistics(\"valid\", val_stats)\n",
        "    train_dataset = RoadLineDataset(\n",
        "        pairs=train_pairs,\n",
        "        image_size=image_size,\n",
        "        augment=True,\n",
        "        mean=train_stats[\"image_mean\"],\n",
        "        std=train_stats[\"image_std\"],\n",
        "    )\n",
        "    val_dataset = RoadLineDataset(\n",
        "        pairs=val_pairs,\n",
        "        image_size=image_size,\n",
        "        augment=False,\n",
        "        mean=train_stats[\"image_mean\"],\n",
        "        std=train_stats[\"image_std\"],\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=args.num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=args.num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    model = UNet(in_channels=3, num_classes=2, base_channels=args.base_channels).to(\n",
        "        device\n",
        "    )\n",
        "    criterion = SegmentationCriterion(\n",
        "        class_counts=train_stats[\"class_counts\"],\n",
        "        w_bg=args.w_bg,\n",
        "        w_fg=args.w_fg,\n",
        "        alpha=args.tv_alpha,\n",
        "        beta=args.tv_beta,\n",
        "        lambda_ce=args.lambda_ce,\n",
        "        lambda_tv=args.lambda_tv,\n",
        "    ).to(device)\n",
        "    optimizer = Adam(\n",
        "        model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay\n",
        "    )\n",
        "    scheduler = ReduceLROnPlateau(\n",
        "        optimizer, mode=\"max\", factor=0.5, patience=3, verbose=True\n",
        "    )\n",
        "    early_stopper = EarlyStopping(patience=args.early_stopping_patience, min_delta=1e-4)\n",
        "    best_iou = 0.0\n",
        "    best_checkpoint: Dict[str, object] | None = None\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        train_ce_sum = 0.0\n",
        "        train_secondary_sum = 0.0\n",
        "        train_samples = 0\n",
        "        train_metrics = MetricsAccumulator(collect_probabilities=False)\n",
        "        for images, masks in train_loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            masks = masks.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(images)\n",
        "            loss, components = criterion(logits, masks, return_components=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            train_ce_sum += components[\"ce\"].item() * images.size(0)\n",
        "            train_secondary_sum += components[\"tversky\"].item() * images.size(0)\n",
        "            train_samples += images.size(0)\n",
        "            train_metrics.update(logits.detach(), masks, threshold=args.val_threshold)\n",
        "        train_loss = running_loss / max(train_samples, 1)\n",
        "        train_ce_loss = train_ce_sum / max(train_samples, 1)\n",
        "        train_secondary_loss = train_secondary_sum / max(train_samples, 1)\n",
        "        train_summary = train_metrics.compute()\n",
        "        model.eval()\n",
        "        val_loss_accum = 0.0\n",
        "        val_ce_sum = 0.0\n",
        "        val_secondary_sum = 0.0\n",
        "        val_samples = 0\n",
        "        val_metrics = MetricsAccumulator(collect_probabilities=True)\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_loader:\n",
        "                images = images.to(device, non_blocking=True)\n",
        "                masks = masks.to(device, non_blocking=True)\n",
        "                logits = model(images)\n",
        "                loss, components = criterion(logits, masks, return_components=True)\n",
        "                val_loss_accum += loss.item() * images.size(0)\n",
        "                val_ce_sum += components[\"ce\"].item() * images.size(0)\n",
        "                val_secondary_sum += components[\"tversky\"].item() * images.size(0)\n",
        "                val_samples += images.size(0)\n",
        "                val_metrics.update(logits, masks, threshold=args.val_threshold)\n",
        "        val_loss = val_loss_accum / max(val_samples, 1)\n",
        "        val_ce_loss = val_ce_sum / max(val_samples, 1)\n",
        "        val_secondary_loss = val_secondary_sum / max(val_samples, 1)\n",
        "        val_summary = val_metrics.compute()\n",
        "        val_iou = float(val_summary[\"foreground_iou\"])\n",
        "        val_accuracy = float(val_summary[\"pixel_accuracy\"])\n",
        "        val_pr_curve = val_metrics.pr_curve(num_thresholds=21)\n",
        "        val_pr_display = val_pr_curve[::5] if len(val_pr_curve) > 0 else []\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "        scheduler.step(val_iou)\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss: {train_loss:.4f} (ce {train_ce_loss:.4f}, tversky {train_secondary_loss:.4f}) | \"\n",
        "            f\"train_fg_iou: {train_summary['foreground_iou']:.4f} | \"\n",
        "            f\"train_fg_dice: {train_summary['foreground_dice']:.4f} | \"\n",
        "            f\"train_prec: {train_summary['precision']:.4f} | \"\n",
        "            f\"train_rec: {train_summary['recall']:.4f} | \"\n",
        "            f\"val_loss: {val_loss:.4f} (ce {val_ce_loss:.4f}, tversky {val_secondary_loss:.4f}) | \"\n",
        "            f\"val_fg_iou: {val_summary['foreground_iou']:.4f}@thr={args.val_threshold:.2f} | \"\n",
        "            f\"val_fg_dice: {val_summary['foreground_dice']:.4f} | \"\n",
        "            f\"val_prec: {val_summary['precision']:.4f} | \"\n",
        "            f\"val_rec: {val_summary['recall']:.4f} | \"\n",
        "            f\"val_boundary_f1: {val_summary['boundary_f1']:.4f} | \"\n",
        "            f\"lr: {current_lr:.2e}\"\n",
        "        )\n",
        "        train_conf = train_summary[\"confusion_matrix\"]\n",
        "        val_conf = val_summary[\"confusion_matrix\"]\n",
        "        print(\n",
        "            f\"    Train confusion (px): TP={train_conf['tp']} FP={train_conf['fp']} \"\n",
        "            f\"FN={train_conf['fn']} TN={train_conf['tn']}\"\n",
        "        )\n",
        "        print(\n",
        "            f\"    Val   confusion (px): TP={val_conf['tp']} FP={val_conf['fp']} \"\n",
        "            f\"FN={val_conf['fn']} TN={val_conf['tn']} | \"\n",
        "            f\"Boundary P/R={val_summary['boundary_precision']:.4f}/{val_summary['boundary_recall']:.4f}\"\n",
        "        )\n",
        "        if val_pr_display:\n",
        "            formatted_curve = \", \".join(\n",
        "                f\"{thr:.2f}:{prec:.2f}/{rec:.2f}/{f1:.2f}\"\n",
        "                for thr, prec, rec, f1 in val_pr_display\n",
        "            )\n",
        "            print(f\"    Val PR curve (thr:prec/rec/f1) -> {formatted_curve}\")\n",
        "        if val_iou > best_iou + 1e-4:\n",
        "            best_iou = val_iou\n",
        "            val_loss_components = {\"ce\": val_ce_loss, \"tversky\": val_secondary_loss}\n",
        "            train_loss_components = {\"ce\": train_ce_loss, \"tversky\": train_secondary_loss}\n",
        "            best_checkpoint = {\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_loss_components\": val_loss_components,\n",
        "                \"val_iou\": val_iou,\n",
        "                \"val_accuracy\": val_accuracy,\n",
        "                \"val_metrics\": val_summary,\n",
        "                \"val_pr_curve\": val_pr_curve,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_loss_components\": train_loss_components,\n",
        "                \"train_metrics\": train_summary,\n",
        "                \"training_args\": vars(args),\n",
        "                \"train_stats\": train_stats,\n",
        "            }\n",
        "            output_dir = Path(args.output_dir)\n",
        "            output_dir.mkdir(parents=True, exist_ok=True)\n",
        "            checkpoint_path = output_dir / \"best_unet.pth\"\n",
        "            torch.save(best_checkpoint, checkpoint_path)\n",
        "            print(f\"Saved new best model to {checkpoint_path} (IoU={val_iou:.4f})\")\n",
        "        if early_stopper.step(val_iou):\n",
        "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
        "            break\n",
        "    if best_checkpoint is not None:\n",
        "        model.load_state_dict(best_checkpoint[\"model_state_dict\"])  # type: ignore[arg-type]\n",
        "        print(\n",
        "            f\"Best validation IoU: {best_checkpoint['val_iou']:.4f} \"\n",
        "            f\"(epoch {best_checkpoint['epoch']})\"\n",
        "        )\n",
        "    else:\n",
        "        print(\n",
        "            \"Training finished without improving validation IoU; no checkpoint saved.\"\n",
        "        )\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "762c61cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def parse_args(argv: Sequence[str] | None = None) -> argparse.Namespace:\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Simplified U-Net pipeline for road line segmentation.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--data-root\",\n",
        "        \"--data_root\",\n",
        "        \"--data_dir\",\n",
        "        dest=\"data_root\",\n",
        "        type=str,\n",
        "        default=\"dataset\",\n",
        "        help=\"Root directory containing train/valid/test folders.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--image-size\",\n",
        "        \"--image_size\",\n",
        "        dest=\"image_size\",\n",
        "        type=int,\n",
        "        nargs=2,\n",
        "        default=(256, 256),\n",
        "        metavar=(\"HEIGHT\", \"WIDTH\"),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\", \"--batch_size\", dest=\"batch_size\", type=int, default=4\n",
        "    )\n",
        "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
        "    parser.add_argument(\n",
        "        \"--learning-rate\",\n",
        "        \"--learning_rate\",\n",
        "        dest=\"learning_rate\",\n",
        "        type=float,\n",
        "        default=1e-3,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--weight-decay\",\n",
        "        \"--weight_decay\",\n",
        "        dest=\"weight_decay\",\n",
        "        type=float,\n",
        "        default=1e-5,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--base-channels\", \"--base_channels\", dest=\"base_channels\", type=int, default=32\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num-workers\",\n",
        "        \"--num_workers\",\n",
        "        dest=\"num_workers\",\n",
        "        type=int,\n",
        "        default=0,\n",
        "        help=\"Set >0 if torch dataloader workers are available.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--early-stopping-patience\",\n",
        "        \"--early_stopping_patience\",\n",
        "        dest=\"early_stopping_patience\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "    )\n",
        "    parser.add_argument(\"--seed\", type=int, default=42)\n",
        "    parser.add_argument(\n",
        "        \"--output-dir\",\n",
        "        \"--output_dir\",\n",
        "        dest=\"output_dir\",\n",
        "        type=str,\n",
        "        default=\"Model\",\n",
        "        help=\"Directory to store checkpoints.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--device\",\n",
        "        type=str,\n",
        "        default=\"auto\",\n",
        "        help=\"Device to use ('auto', 'cpu', 'cuda', 'cuda:0', ...).\",\n",
        "    )\n",
        "    # Loss function is fixed to 'ce+tversky'; see appendix for other variants.\n",
        "    parser.add_argument(\n",
        "        \"--w-bg\",\n",
        "        dest=\"w_bg\",\n",
        "        type=float,\n",
        "        default=1.5,\n",
        "        help=\"Background weight multiplier for CE component.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--w-fg\",\n",
        "        dest=\"w_fg\",\n",
        "        type=float,\n",
        "        default=1.0,\n",
        "        help=\"Foreground weight multiplier for CE component.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--tv-alpha\",\n",
        "        dest=\"tv_alpha\",\n",
        "        type=float,\n",
        "        default=0.7,\n",
        "        help=\"Alpha coefficient for Tversky-based losses (FP penalty).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--tv-beta\",\n",
        "        dest=\"tv_beta\",\n",
        "        type=float,\n",
        "        default=0.3,\n",
        "        help=\"Beta coefficient for Tversky-based losses (FN penalty).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--lambda-ce\",\n",
        "        dest=\"lambda_ce\",\n",
        "        type=float,\n",
        "        default=1.0,\n",
        "        help=\"Weight for cross-entropy component.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--lambda-tv\",\n",
        "        dest=\"lambda_tv\",\n",
        "        type=float,\n",
        "        default=1.0,\n",
        "        help=\"Weight for the Tversky component.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--val-threshold\",\n",
        "        \"--val_threshold\",\n",
        "        dest=\"val_threshold\",\n",
        "        type=float,\n",
        "        default=0.75,\n",
        "        help=\"Probability threshold for validation metrics.\",\n",
        "    )\n",
        "\n",
        "    arguments: List[str]\n",
        "    if argv is None:\n",
        "        arguments = list(sys.argv[1:])\n",
        "    else:\n",
        "        arguments = list(argv)\n",
        "\n",
        "    cleaned_arguments: List[str] = []\n",
        "    skip_next = False\n",
        "    for arg in arguments:\n",
        "        if skip_next:\n",
        "            skip_next = False\n",
        "            continue\n",
        "        # Filter out notebook-injected kernel arguments so argparse works inside Jupyter.\n",
        "        if arg in {\"--f\", \"-f\"}:\n",
        "            skip_next = True\n",
        "            continue\n",
        "        if arg.startswith(\"--f=\") or arg.startswith(\"-f=\"):\n",
        "            continue\n",
        "        cleaned_arguments.append(arg)\n",
        "\n",
        "    return parser.parse_args(cleaned_arguments)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6e5c123",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def resolve_dataset_root(dataset_dir: str = \"dataset\") -> Path:\n",
        "    search_roots = [Path.cwd()]\n",
        "    if \"__file__\" in globals():\n",
        "        search_roots.append(Path(__file__).resolve().parent)\n",
        "    for root in search_roots:\n",
        "        for base in (root, *root.parents):\n",
        "            candidate = base / dataset_dir\n",
        "            if candidate.exists():\n",
        "                return candidate\n",
        "    raise FileNotFoundError(\n",
        "        f\"Unable to locate '{dataset_dir}' directory from {Path.cwd()}\"\n",
        "    )\n",
        "\n",
        "data_root_path = resolve_dataset_root()\n",
        "\n",
        "notebook_args = [\n",
        "    \"--data_root\", str(data_root_path),\n",
        "    \"--epochs\", \"30\",\n",
        "    \"--batch_size\", \"4\",\n",
        "    \"--device\", \"cuda\",\n",
        "    \"--learning_rate\", \"1e-3\",\n",
        "    \"--weight_decay\", \"1e-5\",\n",
        "    \"--output_dir\", \"Model\",\n",
        "    \"--val_threshold\", \"0.75\",\n",
        "]\n",
        "\n",
        "args = parse_args(notebook_args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbab942c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[train] samples: 751\n",
            "[train] image mean: [0.4312999436802934, 0.44875362059089063, 0.4239335453879341]\n",
            "[train] image std:  [0.23804450725234275, 0.23794269857501038, 0.24852641466394798]\n",
            "[train] class counts (background, foreground): [48278140, 939396]\n",
            "[train] class frequencies: [0.9809133882687666, 0.019086611731233355]\n",
            "[train] positive pixel ratio mean=0.0191 std=0.0214 min=0.0000 p50=0.0131 max=0.2426\n",
            "[train] positive pixel ratio histogram (bin:count) -> 0.05:743, 0.15:7, 0.25:1, 0.35:0, 0.45:0, 0.55:0, 0.65:0, 0.75:0, 0.85:0, 0.95:0\n",
            "\n",
            "[valid] samples: 216\n",
            "[valid] image mean: [0.4364930471681334, 0.45583993368954573, 0.42995701616423]\n",
            "[valid] image std:  [0.2397509437434158, 0.23891895409026215, 0.25044919758865736]\n",
            "[valid] class counts (background, foreground): [13868481, 287295]\n",
            "[valid] class frequencies: [0.9797047509087456, 0.02029524909125434]\n",
            "[valid] positive pixel ratio mean=0.0203 std=0.0202 min=0.0012 p50=0.0133 max=0.1521\n",
            "[valid] positive pixel ratio histogram (bin:count) -> 0.05:214, 0.15:2, 0.25:0, 0.35:0, 0.45:0, 0.55:0, 0.65:0, 0.75:0, 0.85:0, 0.95:0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mtrain_and_validate\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     78\u001b[39m loss.backward()\n\u001b[32m     79\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * images.size(\u001b[32m0\u001b[39m)\n\u001b[32m     81\u001b[39m train_ce_sum += components[\u001b[33m\"\u001b[39m\u001b[33mce\u001b[39m\u001b[33m\"\u001b[39m].item() * images.size(\u001b[32m0\u001b[39m)\n\u001b[32m     82\u001b[39m train_secondary_sum += components[\u001b[33m\"\u001b[39m\u001b[33mtversky\u001b[39m\u001b[33m\"\u001b[39m].item() * images.size(\u001b[32m0\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "train_and_validate(args)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
